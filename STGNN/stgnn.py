# coding: utf-8
# Copyright (c) Antfin, Inc. All rights reserved.
# Author: shuoyang

from __future__ import absolute_import

import logging

import tensorflow as tf

from alps.common.model.base_model import BaseModel
from alps.graph_embedding.model.graph_embedding_layer import GraphEmbedding
from alps.graph_embedding.constants import GraphConfigConstants
from alps.graph_embedding.utils.self_attention_mechanism import self_attention_mechanism
from alps.graph_embedding.utils.w_variable import glorot


class TemporalGraphEmbeddingModel(BaseModel):
    r"""
        Spatial-Temporal aware Graph Neural Network
        ST-GNN models aggregate information in both spatial and temporal dimensions,
        The input features of ST-GNN model are consist of 2 * time_step columns:
            id_step0, id_step1, ..., id_step{time_step-1}: node id in each graph snapshot
            graph_feature_step0, ..., graph_feature_step0{time_step-1}:
                feature generated by graphflat in each graph snapshot
        Training example files can be found in examples/graph_embedding/k8s_stgnn
        There are seven kinds of temporal-aware aggregators:
            self_attention, lstm, gru, attention, self_attention, star, stgnn

    """

    def __init__(self, config):
        super(TemporalGraphEmbeddingModel, self).__init__(config)
        self._config = self.config
        self.average_loss = 0.0
        self.total_loss = 0.0
        self.train_loss_steps = self._config.get('train_loss_steps', 1)
        self._customer_outputs = {}

    def _temporal_graph_model(self, graph_input):
        model_def = self._config.model_def
        time_step = model_def.get_int('time_step', 1)
        embedding_size = model_def.get_int('embedding_size', 32)
        graph_embedding_algorithm_class = \
            model_def.get("graph_embedding_algorithm_class",
                          "alps.graph_embedding.model.algorithm.geniepath.GeniePath")
        graph_feature_process_class = "alps.graph_embedding.graph_feature.BaseGraphInput"

        with tf.variable_scope(GraphConfigConstants.TEMPORAL_EMBEDDING_ALGORITHM_SCOPE_NAME, reuse=tf.AUTO_REUSE):
            node_embedd_list = []
            for i in range(time_step):
                graph_inputs_node = GraphEmbedding.transform_graph_feature_to_graph_input(
                    graph_feature_process_class,
                    ids=graph_input['id_step{}'.format(i)],
                    graph_feature=graph_input['graph_feature_step{}'.format(i)],
                    conf=self._config
                )
                node_embed_result = GraphEmbedding.build_graph_embedding(
                    graph_embedding_algorithm_class,
                    ids=graph_input['id_step{}'.format(i)],
                    graph_input=graph_inputs_node,
                    conf=self._config)
                node_embedd = node_embed_result[GraphConfigConstants.BUILD_GRAPH_EMBEDDING_OUTPUT_KEY_EMBEDDINGS]
                node_embedd_list.append([node_embedd])
            temporal_aggregator = model_def.get('temporal_aggregator', None)
            if temporal_aggregator == 'self_attention':
                aggregator = SelfAttentionTemporalAggregator(model_def)
            elif temporal_aggregator == 'memory':
                aggregator = MemoryTemporalAggregator(model_def)
            elif temporal_aggregator == 'lstm':
                aggregator = LSTMTemporalAggregator(model_def)
            elif temporal_aggregator == 'gru':
                aggregator = GRUTemporalAggregator(model_def)
            elif temporal_aggregator == 'attention':
                aggregator = AttentionTemporalAggregator(model_def)
            elif temporal_aggregator == 'star':
                aggregator = STARTemporalAggregator(model_def)
            else:
                aggregator = STGNNAggregator(model_def)
            output_embedd = aggregator.aggregate(node_embedd_list)
            output_embedd = tf.keras.layers.Dense(embedding_size)(output_embedd)
        return output_embedd, aggregator.stable_regular

    def build_model(self, inputs, labels):
        """Build model.

        Args:
            inputs: inputs is a dict, key is the feature_name you
                config in the x, value is the placeholder,
                which by default is keras Input
            labels: labels is a dict, key is the feature_name you
                config in the y, value is the placeholder
        Returns:
            the forward output op
        """

        self._loss = 0
        labels = labels['label']
        # 1. generate graph embeddings for node1 and node2

        output_embedd, stable_regularizaiton = self._temporal_graph_model(inputs)

        model_def = self._config.model_def

        num_class = model_def.get('num_class', 2)
        graph_embedding_prediction_class = \
            model_def.get(GraphConfigConstants.PREDICTION_CLASS, GraphConfigConstants.PREDICTION_CLASS_DEFAULT)
        loss_function_class = \
            model_def.get(GraphConfigConstants.LOSS_CLASS, GraphConfigConstants.LOSS_CLASS_DEFAULT)
        prediction_result, prediction_for_loss = GraphEmbedding.predict(graph_embedding_prediction_class,
                                                                        output_embedd,
                                                                        num_class,
                                                                        self._config,
                                                                        inputs=inputs)
        loss_train, accuracy, loss_print = GraphEmbedding.loss(loss_function_class, prediction_for_loss, labels,
                                                               self._config)
        stable_regularizaiton = model_def.get('stable_regularizaiton', False)
        if stable_regularizaiton:
            loss_train += stable_regularizaiton

        self._loss = loss_train
        self._loss_print = loss_print
        self._accuracy = accuracy
        self._embedding = output_embedd
        self._prediction_result = prediction_result
        self._labels = labels

        if stable_regularizaiton is not None and stable_regularizaiton != 0:
            self._loss += stable_regularizaiton

        self._customer_outputs.update({
            'prediction': self._prediction_result,
            'selected_embedding': self._embedding,
            'labels': self._labels
        })

        return {"prediction":self._prediction_result, "embedding":self._embedding}

    def on_step_end(self, sess, local_step, results):
        if local_step % self.train_loss_steps != 0:
            self.total_loss += results["loss_print"]
        else:
            self.total_loss += results["loss_print"]
            self.average_loss = self.total_loss / self.train_loss_steps
            logging.info(
                "local step = %d, %d steps train_loss = %f " % (local_step, self.train_loss_steps, self.average_loss))
            self.total_loss = 0.0

    def get_loss(self, **kwargs):
        return self._loss

    def get_metrics(self, **kwargs):
        metrics = {'accuracy': self._accuracy}
        if self._loss_print is not None:
            metrics.update({'loss_print': self._loss_print})
        return metrics

    def customer_outputs(self):
        self._customer_outputs.update({"embedding": self._embedding})
        return self._customer_outputs

    def get_prediction_result(self, **kwargs):
        return self._prediction_result

    def get_summary_op(self):
        # until alps-common support tf.dataset more robustly
        tf.summary.scalar('train_loss', self._loss)
        tf.summary.scalar('train_accuracy', self._accuracy)
        return tf.summary.merge_all(), None


class TemporalAggregator:
    def __init__(self, model_def):
        self.stable_regular = 0.0
        self.model_def = model_def

    def aggregate(self, node_embedd_list):
        pass


class GRUTemporalAggregator(TemporalAggregator):

    def aggregate(self, node_embedd_list):
        stable_regularizaiton = self.model_def.get('stable_regularizaiton', False)
        lambda_stable = self.model_def.get('lambda_stable', 0.0)
        embedding_size = node_embedd_list[0][0].get_shape().as_list()[-1]
        vars_wrh = glorot([embedding_size, embedding_size], 'w_t_r_h')
        vars_wrx = glorot([embedding_size, embedding_size], 'w_t_r_x')
        vars_wzh = glorot([embedding_size, embedding_size], 'w_t_z_h')
        vars_wzx = glorot([embedding_size, embedding_size], 'w_t_z_x')
        vars_whh = glorot([embedding_size, embedding_size], 'w_t_h_h')
        vars_whx = glorot([embedding_size, embedding_size], 'w_t_h_x')
        vars_br = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_r')
        vars_bz = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_z')
        vars_bh = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_h')

        hidden_state = 0.0
        for i, embedding in enumerate(node_embedd_list):
            embedding = embedding[0]
            if i == 0:
                z_gate = tf.nn.sigmoid(tf.matmul(embedding, vars_wzx) + vars_bz)
                hprime = tf.nn.tanh(tf.matmul(embedding, vars_whx) + vars_bh)
                hidden_state = z_gate * hprime
            else:
                z_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wzx)
                    + tf.matmul(hidden_state, vars_wzh)
                    + vars_bz
                )
                r_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wrx)
                    + tf.matmul(hidden_state, vars_wrh)
                    + vars_br
                )
                hprime = tf.nn.tanh(
                    tf.matmul(embedding, vars_whx)
                    + tf.matmul(hidden_state * r_gate, vars_whh)
                    + vars_bh
                )
                hidden_state = (1 - z_gate) * hidden_state + z_gate * hprime

            if stable_regularizaiton and lambda_stable > 0.0:
                self.stable_regular += 1 / (i + 1) * lambda_stable * tf.reduce_mean(z_gate)

        return hidden_state


class LSTMTemporalAggregator(TemporalAggregator):

    def aggregate(self, node_embedd_list):

        # embedding_size = self.model_def.get_int('embedding_size', 8)
        embedding_size = node_embedd_list[0][0].get_shape().as_list()[-1]
        vars_wih = glorot([embedding_size, embedding_size], 'w_t_i_h')
        vars_wix = glorot([embedding_size, embedding_size], 'w_t_i_x')
        vars_woh = glorot([embedding_size, embedding_size], 'w_t_o_h')
        vars_wox = glorot([embedding_size, embedding_size], 'w_t_o_x')
        vars_wfh = glorot([embedding_size, embedding_size], 'w_t_f_h')
        vars_wfx = glorot([embedding_size, embedding_size], 'w_t_f_x')
        vars_wch = glorot([embedding_size, embedding_size], 'w_t_c_h')
        vars_wcx = glorot([embedding_size, embedding_size], 'w_t_c_x')
        vars_bf = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_f')
        vars_bi = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_i')
        vars_bo = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_o')
        vars_bc = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_c')

        hidden_state = 0.0
        C = 0.0
        for i, embedding in enumerate(node_embedd_list):
            embedding = embedding[0]
            if i == 0:
                input_gate = tf.nn.sigmoid(tf.matmul(embedding, vars_wix) + vars_bi)
                output_gate = tf.nn.sigmoid(tf.matmul(embedding, vars_wox) + vars_bo)
                C_update = tf.nn.tanh(tf.matmul(embedding, vars_wcx) + vars_bc)
                C = input_gate * C_update
            else:
                input_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wix) + tf.matmul(hidden_state, vars_wih) + vars_bi
                )
                forget_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wfx) + tf.matmul(hidden_state, vars_wfh) + vars_bf
                )
                output_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wfx) + tf.matmul(hidden_state, vars_woh) + vars_bo
                )
                C_update = tf.nn.tanh(
                    tf.matmul(embedding, vars_wcx) + tf.matmul(hidden_state, vars_wch) + vars_bc
                )
                C = tf.add(tf.multiply(forget_gate, C), tf.multiply(input_gate, C_update))
            hidden_state = tf.multiply(output_gate, tf.nn.tanh(C))

        return hidden_state


class AttentionTemporalAggregator(TemporalAggregator):

    def aggregate(self, node_embedd_list):
        embedding_size = node_embedd_list[0][0].get_shape().as_list()[-1]

        vars_wcur = glorot([embedding_size, embedding_size], 'w_t_cur')
        vars_bcur = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_cur')
        vars_wpre = glorot([embedding_size, embedding_size], 'w_t_pre')
        vars_bpre = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_pre')
        vars_watt = glorot([embedding_size, 1], 'w_t_att')
        vars_batt = tf.get_variable(initializer=tf.zeros([1], dtype=tf.float32), name='b_t_att')

        cur_embbed = tf.matmul(node_embedd_list[-1][0], vars_wcur) + vars_bcur
        # T * N * 1
        att_weight_ini_list = []
        for i, embedding in enumerate(node_embedd_list):
            embedding = tf.matmul(embedding[0], vars_wpre) + vars_bpre
            att_weight_ini = tf.matmul(cur_embbed + embedding, vars_watt) + vars_batt
            att_weight_ini_list.append(att_weight_ini)
        # N * T
        att_weight = tf.nn.softmax(tf.concat(att_weight_ini_list, 1), 1)
        # N * T * 1
        att_weight = tf.expand_dims(att_weight, 2)
        # T * N * D -> N * T * D
        node_embedd_sequence = tf.transpose(tf.concat(node_embedd_list, 0), perm=[1, 0, 2])
        # N * T * D  mul N * T * 1 -> N * T * D -> N * D
        hidden_state = tf.reduce_sum(node_embedd_sequence * att_weight, 1)
        return hidden_state


class STARTemporalAggregator(TemporalAggregator):

    def aggregate(self, node_embedd_list):
        embedding_size = node_embedd_list[0][0].get_shape().as_list()[-1]
        vars_wrh = glorot([embedding_size, embedding_size], 'w_t_r_h')
        vars_wrx = glorot([embedding_size, embedding_size], 'w_t_r_x')
        vars_wzh = glorot([embedding_size, embedding_size], 'w_t_z_h')
        vars_wzx = glorot([embedding_size, embedding_size], 'w_t_z_x')
        vars_whh = glorot([embedding_size, embedding_size], 'w_t_h_h')
        vars_whx = glorot([embedding_size, embedding_size], 'w_t_h_x')
        vars_br = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_r')
        vars_bz = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_z')
        vars_bh = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_h')

        hidden_state = 0.0
        hidden_state_list = []
        for i, embedding in enumerate(node_embedd_list):
            embedding = embedding[0]
            if i == 0:
                z_gate = tf.nn.sigmoid(tf.matmul(embedding, vars_wzx) + vars_bz)
                hprime = tf.nn.tanh(tf.matmul(embedding, vars_whx) + vars_bh)
                hidden_state = z_gate * hprime
            else:
                z_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wzx)
                    + tf.matmul(hidden_state, vars_wzh)
                    + vars_bz
                )
                r_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wrx)
                    + tf.matmul(hidden_state, vars_wrh)
                    + vars_br
                )
                hprime = tf.nn.tanh(
                    tf.matmul(embedding, vars_whx)
                    + tf.matmul(hidden_state * r_gate, vars_whh)
                    + vars_bh
                )
                hidden_state = (1 - z_gate) * hidden_state + z_gate * hprime
            hidden_state_list.append([hidden_state])

        vars_wcur = glorot([embedding_size, embedding_size], 'w_t_cur')
        vars_bcur = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_cur')
        vars_wpre = glorot([embedding_size, embedding_size], 'w_t_pre')
        vars_bpre = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_pre')
        vars_watt = glorot([embedding_size, 1], 'w_t_att')
        vars_batt = tf.get_variable(initializer=tf.zeros([1], dtype=tf.float32), name='b_t_att')

        cur_embbed = tf.matmul(hidden_state_list[-1][0], vars_wcur) + vars_bcur
        # T * N * 1
        att_weight_ini_list = []
        for i, embedding in enumerate(hidden_state_list):
            embedding = tf.matmul(embedding[0], vars_wpre) + vars_bpre
            att_weight_ini = tf.matmul(cur_embbed + embedding, vars_watt) + vars_batt
            att_weight_ini_list.append(att_weight_ini)

        # N * T
        att_weight = tf.nn.softmax(tf.concat(att_weight_ini_list, 1), 1)
        # N * T * 1
        att_weight = tf.expand_dims(att_weight, 2)
        # T * N * D -> N * T * D
        node_embedd_sequence = tf.transpose(tf.concat(hidden_state_list, 0), perm=[1, 0, 2])
        # N * T * D  mul N * T * 1 -> N * T * D -> N * D
        hidden_state = tf.reduce_sum(node_embedd_sequence * att_weight, 1)
        return hidden_state


class MemoryTemporalAggregator(TemporalAggregator):

    def aggregate(self, node_embedd_list):
        embedding_size = node_embedd_list[0][0].get_shape().as_list()[-1]
        vars_wrh = glorot([embedding_size, embedding_size], 'w_t_r_h')
        vars_wrx = glorot([embedding_size, embedding_size], 'w_t_r_x')
        vars_br = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_r')
        hidden_state = 0.0
        for i, embedding in enumerate(node_embedd_list):
            embedding = embedding[0]
            if i == 0:
                hidden_state = embedding
            else:
                r_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wrx)
                    + tf.matmul(hidden_state, vars_wrh)
                    + vars_br
                )
                hidden_state = (1 - r_gate) * hidden_state + r_gate * embedding
        return hidden_state


class SelfAttentionTemporalAggregator(TemporalAggregator):

    def aggregate(self, node_embedd_list):
        time_step = self.model_def.get_int('time_step', 1)
        # embedding_size = self.model_def.get_int('embedding_size', 8)
        num_head = self.model_def.get_int('num_head', 8)
        embedding_size = node_embedd_list[0][0].get_shape().as_list()[-1]
        node_embedd_sequence = tf.concat(node_embedd_list, 0)
        node_embedd_sequence = tf.transpose(node_embedd_sequence, perm=[1, 0, 2])
        output_embedd = self_attention_mechanism(
            input_tensor=node_embedd_sequence,
            seq_len=time_step,
            num_head=num_head,
            size_per_head=embedding_size,
            use_pos_embedd=True
        )
        return output_embedd


class STGNNAggregator(TemporalAggregator):
    def aggregate(self, node_embedd_list):

        # embedding_size = self.model_def.get_int('embedding_size', 8)
        embedding_size = node_embedd_list[0][0].get_shape().as_list()[-1]
        vars_wih = glorot([embedding_size, embedding_size], 'w_t_i_h')
        vars_wix = glorot([embedding_size, embedding_size], 'w_t_i_x')
        vars_woh = glorot([embedding_size, embedding_size], 'w_t_o_h')
        vars_wox = glorot([embedding_size, embedding_size], 'w_t_o_x')
        vars_wfh = glorot([embedding_size, embedding_size], 'w_t_f_h')
        vars_wfx = glorot([embedding_size, embedding_size], 'w_t_f_x')
        vars_wch = glorot([embedding_size, embedding_size], 'w_t_c_h')
        vars_wcx = glorot([embedding_size, embedding_size], 'w_t_c_x')
        vars_bf = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_f')
        vars_bi = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_i')
        vars_bo = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_o')
        vars_bc = tf.get_variable(initializer=tf.zeros([embedding_size], dtype=tf.float32), name='b_t_c')

        hidden_state = 0.0
        C = 0.0
        embedd_list = []
        for i, embedding in enumerate(node_embedd_list):

            embedding = embedding[0]
            if i == 0:
                input_gate = tf.nn.sigmoid(tf.matmul(embedding, vars_wix) + vars_bi)
                output_gate = tf.nn.sigmoid(tf.matmul(embedding, vars_wox) + vars_bo)
                C_update = tf.nn.tanh(tf.matmul(embedding, vars_wcx) + vars_bc)
                C = input_gate * C_update
            else:
                input_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wix) + tf.matmul(hidden_state, vars_wih) + vars_bi
                )
                forget_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wfx) + tf.matmul(hidden_state, vars_wfh) + vars_bf
                )
                output_gate = tf.nn.sigmoid(
                    tf.matmul(embedding, vars_wfx) + tf.matmul(hidden_state, vars_woh) + vars_bo
                )
                C_update = tf.nn.tanh(
                    tf.matmul(embedding, vars_wcx) + tf.matmul(hidden_state, vars_wch) + vars_bc
                )
                C = tf.add(tf.multiply(forget_gate, C), tf.multiply(input_gate, C_update))
            hidden_state = tf.multiply(output_gate, tf.nn.tanh(C))
            embedd_list.append([embedding])
            embedd_list.append([hidden_state])

        num_head = self.model_def.get_int('num_head', 8)
        time_step = self.model_def.get_int('time_step', 1)
        node_embedd_sequence = tf.concat(embedd_list, 0)
        node_embedd_sequence = tf.transpose(node_embedd_sequence, perm=[1, 0, 2])
        output_embedd = self_attention_mechanism(
            input_tensor=node_embedd_sequence,
            seq_len=time_step * 2,
            num_head=num_head,
            size_per_head=embedding_size,
            use_pos_embedd=True
        )
        return output_embedd