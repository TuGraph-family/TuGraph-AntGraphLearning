# user-define

node_feature_dim = 60000
edge_feature_dim = 60000
num_class = 2

# your type of task
task_type = "exclusive-label"
prediction_type = "node"

# your model hyperparameters
pool = "geniepath"
lr = 0.0001
l_2 = 0.000
residual = 1
keep_prob = 1.0 # 1 - dropout probability
with_edge = True
max_bp_iter = 2
embedding_size = 64
num_head = 4
neighbor_limit = -1
msg_aggr_strategy: "MEAN"
print_debug_info: False

# epoch batch_size print_info
epoch = 4
train_batch_size = 512
test_batch_size = 512
validation_steps = 100000
log_steps = 3


output_model_path = "oss://cmps-model/370529/qytppd_iii_v2_1121_layer2_stgnn_step3_epoch4"
checkpoint_path = "oss://cmps-model/370529/qytppd_iii_v2_1121_layer2_stgnn_step3_epoch4"
deploy_path = "oss://cmps-model/370529/yx_mybk_rm_dev/qytppd_iii_v2_1121_layer2_stgnn_step3_epoch4.tar.gz"


user = "zhumeiqi.zmq"
name = "meiqi_epd_gta_v1"
# end user-define


schema = [id_step0, id_step1, id_step2, graph_feature_step0, graph_feature_step1, graph_feature_step2, label]

model_def {
    model = base_model.TemporalGraphEmbeddingModel  # model should implements BaseModel api, default is dnn using layer_dims
    graph_embedding_algorithm_class = alps.graph_embedding.model.algorithm.geniepath.GeniePath
    graph_embedding_prediction_class = alps.graph_embedding.model.prediction.classification_prediction.ClassificationPrediction
    loss_function_class = alps.graph_embedding.model.loss.multi_label_classification_loss.MultiLabelClassificationLoss

    temporal_aggregator = stgnn
    time_step = 3
    stable_regularizaiton = False

    datasetx.enable_aistudio_reader = true
    datasetx.aistudio_reader_num_processes = 2
    datasetx.aistudio_reader_queue_size=512
    # loss = cross_entropy  # default is cross_entropy
    optimizer: {
        name: adam,
        learning_rate: ${lr}
    } # default optimizer

    dataset {
        enable: False
    }


    schema = ${schema}
    node_feature_dim: ${node_feature_dim}
    edge_feature_dim: ${edge_feature_dim}
    num_class: ${num_class}

    neighbor_limit = ${neighbor_limit} # max num of queried neighbors
    embedding_size: ${embedding_size}
    max_bp_iter: ${max_bp_iter}
    prediction: ${prediction_type}
    msg_aggr_strategy: ${msg_aggr_strategy}
    print_debug_info: ${print_debug_info}
    l2 =  ${l_2} # FEEDME
    geniepath {
        pool = ${pool} # geniepath-lazy, gat
        residual = ${residual} # FEEDME
        task_type = ${task_type} # "exclusive-label"
        #l2 =  ${l_2} # not used, use model_def.l2 instead
        keep_prob = ${keep_prob} # 1 - dropout probability
        with_edge = ${with_edge}
    }
}

Train { # Operator
    model_type: tf
    batch_size = ${train_batch_size}   # not used, but still need to provide
    model_name: tf_model
    fix_sparse_shape = False

    output_dir: "./output_dir"

    datasetx.enable_aistudio_reader = true
    datasetx.aistudio_reader_num_processes = 2
    datasetx.aistudio_reader_queue_size=512

    log_steps: ${log_steps}
    save_steps: ${validation_steps}
    # default is 1, will take effect only when input.train.max_iter_num < 0
    epoch: ${epoch}
    max_iter_num: -1
    graph_data: "graphflat://"

    worker {
        op = alps.graph_embedding.trainer.GraphTrainer
        trainer_hooks = alps.graph.hooks.DeployModelHook
    }

    dataset {
        enable: False
    }

    model_def = ${model_def}
    
    validation {
        op = alps.graph_embedding.validation.ks_validation.KSValidation
        parallel = True
        include_metrics = True
    }

    x: [
        {feature_name: id_step0, type: dense, dtype: string, shape:[1] }
        {feature_name: id_step1, type: dense, dtype: string, shape:[1] }
        {feature_name: id_step2, type: dense, dtype: string, shape:[1] }
        {feature_name: graph_feature_step0, type: dense, dtype: string, shape: [1]}
        {feature_name: graph_feature_step1, type: dense, dtype: string, shape: [1]}
        {feature_name: graph_feature_step2, type: dense, dtype: string, shape: [1]}
       ]
    y: [
        {feature_name: label, type: dense, shape: [${num_class}], separator: " "}
       ]

    output.model {
        path = ${output_model_path}
    }

    input.checkpoint {
        path = ${checkpoint_path}
    }

    input.train {
        schema = ${schema}
        shuffle = True
        batch_size = ${train_batch_size}
        use_manage_dict = True
    }

    input.test {
        schema = ${schema}
        batch_size = ${test_batch_size}
        shuffle = False
        use_manage_dict = True
    }

    exec {
        cluster = alipay_et15
        deploy_tag = tensorflow
        worker.memory = 30000
        worker.core = 8
        worker.num = 60
        worker.disk_quota = 30000
        ps.num = 2
        ps.core = 2
        ps.memory = 30000 # Unit: MB
        image ="reg.docker.alibaba-inc.com/alipay-alps/alps-runtime:cpu-tf1.13.1-py3-v5"
    }
}


Arks {
     exec {
        image ="reg.docker.alibaba-inc.com/alipay-alps/alps-runtime:cpu-tf1.13.1-py3-v5"
        cluster: "alipay_et15"
        queue: ""
     }
     model_name = tf_model
     worker  {
         op = alps.common.tools.DeployModel   # default is BaseTrainer
     }
    deploy {
        compress = true
        path = ${deploy_path}
    }
}
